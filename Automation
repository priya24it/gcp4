import os
import yaml
from google.cloud import bigquery

# Set up BigQuery client (Auto-login method assumes authentication via gcloud CLI)
client = bigquery.Client()

# Query to get distinct table names from INFORMATION_SCHEMA.PARTITIONS
query = """
SELECT DISTINCT table_name
FROM `your_project_id.your_dataset.INFORMATION_SCHEMA.PARTITIONS`
"""

# Run query
query_job = client.query(query)
table_names = [row["table_name"] for row in query_job]

# Define the base SQL template
sql_template = """{% set table_mapping = {
    '{table_name}': '{table_name}_backup'
} %}

{{ compare_latest_partition_mapped(table_mapping) }}
"""

# Directory where SQL files will be created
output_dir = "generated_sql_files"
os.makedirs(output_dir, exist_ok=True)

# Generate .SQL files for each table
for table in table_names:
    file_path = os.path.join(output_dir, f"{table}.sql")
    with open(file_path, "w") as sql_file:
        sql_file.write(sql_template.format(table_name=table))

# Path to sources.yaml file
sources_yaml_path = "sources.yaml"

# Load or create sources.yaml
if os.path.exists(sources_yaml_path):
    with open(sources_yaml_path, "r") as yaml_file:
        sources_data = yaml.safe_load(yaml_file)
else:
    sources_data = {"sources": []}

# Add new tables to sources.yaml
source_name = "bigquery_source"  # Change this as needed
existing_source = next((s for s in sources_data["sources"] if s["name"] == source_name), None)

if not existing_source:
    existing_source = {"name": source_name, "tables": []}
    sources_data["sources"].append(existing_source)

existing_tables = {t["name"] for t in existing_source["tables"]}

# Append new tables if not already present
for table in table_names:
    if table not in existing_tables:
        existing_source["tables"].append({"name": table})

# Save back to sources.yaml
with open(sources_yaml_path, "w") as yaml_file:
    yaml.dump(sources_data, yaml_file, default_flow_style=False)

print(f"Generated SQL files in '{output_dir}' and updated 'sources.yaml'.")
